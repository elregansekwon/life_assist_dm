from langchain_openai import ChatOpenAI
from langchain.prompts import PromptTemplate
from langchain_core.output_parsers import StrOutputParser

# httpx ë¡œê·¸ ì–µì œ (rqt ë©”ëª¨ë¦¬ ê³¼ë¶€í•˜ ë°©ì§€)
import logging as std_logging
std_logging.getLogger("httpx").setLevel(std_logging.WARNING)
std_logging.getLogger("httpcore").setLevel(std_logging.WARNING)


class LifeAssistant:
<<<<<<< HEAD
    def __init__(self, model_name="gpt-4o-mini"):
        self.llm = ChatOpenAI(model=model_name,
                              temperature=0.4)
=======
    def __init__(self, model_name="gpt-4o-mini-2024-07-18"):
        # LLM í˜¸ì¶œ timeout ì„¤ì • (10ì´ˆ) - ROS2 ì„œë¹„ìŠ¤ ì‘ë‹µ ì§€ì—° ë°©ì§€
        self.llm = ChatOpenAI(
            model=model_name,
            temperature=0.4,
            timeout=10.0,  # 10ì´ˆ timeout - rqt ì„œë¹„ìŠ¤ ì‘ë‹µ ì§€ì—° ë°©ì§€
            max_retries=1  # ì¬ì‹œë„ ìµœì†Œí™”
        )
>>>>>>> 9f3045d (2025-11-06 ìˆ˜ì • ì‚¬í•­ ë°˜ì˜)
        self.output_parser = StrOutputParser()
        self.prompt = PromptTemplate.from_template(
            "ì…ë ¥ëœ ë‚´ìš©ì€ ì‚¬ëŒì´ ë¡œë´‡ì—ê²Œ ìš”ì²­í•œ ëª…ë ¹ì…ë‹ˆë‹¤."
            "ë¡œë´‡ì€ ì‚¬ëŒì—ê²Œ [ì¸ì§€], [ì •ì„œ], [ë¬¼ë¦¬ì  ì§€ì›] ì„œë¹„ìŠ¤ë¥¼ ì œê³µí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
            "ë¡œë´‡ì´ ì„œë¹„ìŠ¤ë¥¼ ì œê³µí•˜ê¸° ìœ„í•´ ì–´ë–¤ ì‘ì—…ì„ ìˆ˜í–‰í• ì§€ [ì¸ì§€], [ì •ì„œ], [ë¬¼ë¦¬ì  ì§€ì›]ìœ¼ë¡œ ë¶„ë¥˜ë˜ì–´ì•¼ í•©ë‹ˆë‹¤."
            "[ì¸ì§€] ì„œë¹„ìŠ¤: ì‚¬ëŒì˜ ê¸°ì–µë ¥, ë°˜ë³µë˜ëŠ” í–‰ìœ„, ìŠ¤ì¼€ì¤„ ë“±ê³¼ ê´€ë ¨ëœ ì„œë¹„ìŠ¤ì…ë‹ˆë‹¤."
            "[ì •ì„œ] ì„œë¹„ìŠ¤: ì‚¬ëŒì˜ ê°ì •ê³¼ ê´€ë ¨ëœ ì„œë¹„ìŠ¤ì´ë©°, ì¼ìƒì ì¸ ëŒ€í™”ë‚˜ ìŠ¤íŠ¸ë ˆìŠ¤ ê´€ë¦¬, ë‚ ì”¨ ì•ˆë‚´ ë“± ë³´í†µì˜ ëŒ€í™”ì— í•´ë‹¹í•©ë‹ˆë‹¤."
            "[ë¬¼ë¦¬ì  ì§€ì›] ì„œë¹„ìŠ¤: ì‚¬ëŒì´ ë¡œë´‡ì—ê²Œ ì–´ë–¤ ë¬¼ê±´ì„ ê°–ë‹¤ë‹¬ë¼ê±°ë‚˜, ì°¾ì•„ë‹¬ë¼ê³  í•˜ëŠ” ë“± ë¡œë´‡ì´ ë¬¼ì²´ë¥¼ ì¡°ì‘í•˜ê±°ë‚˜ ê´€ì°°í•´ì•¼í•˜ëŠ” ì„œë¹„ìŠ¤ì— í•´ë‹¹í•©ë‹ˆë‹¤."
            "ì‚¬ëŒì˜ ëª…ë ¹ì„ ë³´ê³  [ì¸ì§€],[ì •ì„œ],[ë¬¼ë¦¬ì  ì§€ì›] ì„œë¹„ìŠ¤ ì¤‘ì—ì„œ ê³¨ë¼ì£¼ì„¸ìš”. "
            "ì„œë¹„ìŠ¤ ê²°ê³¼ê°€ [ì¸ì§€], [ì •ì„œ]ì¸ ê²½ìš° ëª…ë ¹ì— ì í•©í•œ ëŒ€ë‹µì„ í•´ì£¼ê³ , "
            "[ë¬¼ë¦¬ì  ì§€ì›]ì¸ ê²½ìš° ëª…ë ¹ì„ ìŠì–´ì£¼ë©° ì‹¤í–‰í• ì§€ ë¬¼ì–´ë³´ëŠ” ëŒ€ë‹µì„ ìƒì„±í•œ í›„, ì´ˆê¸° ëª…ë ¹ì„ ì˜ì–´ë¡œ ë²ˆì—­í•´ì£¼ì„¸ìš”."
            "[ë¬¼ë¦¬ì  ì§€ì›]ì˜ ì‘ë‹µì€ ìˆ˜í–‰í• ì§€ ë¬»ëŠ” ê²ƒê³¼ ì˜ì–´ë¡œ ë²ˆì—­í•œ ë¬¸ì¥ì„ '/'ë¡œ êµ¬ë¶„í•´ì„œ ì‘ë‹µí•´ì£¼ì„¸ìš”."
            "ì‘ë‹µ ì˜ˆì‹œëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤. '[ì¸ì§€] ì˜¤ëŠ˜ ê°ê¸°ì•½ ë“œì…¨ë‚˜ìš”?', '[ì •ì„œ] ì˜¤ëŠ˜ ë‚ ì”¨ê°€ ì¢‹ì•„ìš”', '[ë¬¼ë¦¬ì ì§€ì›] ë¬¼ ê°–ë‹¤ ë“œë¦´ê¹Œìš”? / Would you like me to bring you some water?"
            "ìœ„ ì˜ˆì‹œì²˜ëŸ¼ ì„œë¹„ìŠ¤ ì¢…ë¥˜ê°€ ë¬¸ì¥ ì•, ì‘ë‹µì´ ì„œë¹„ìŠ¤ ì¢…ë¥˜ ë‹¤ìŒì— ë°°ì¹˜ëœ ëŒ€ë‹µì„ í•´ì•¼í•©ë‹ˆë‹¤."
            "ë§Œì•½ ì‚¬ìš©ì ëª…ë ¹ì—ì„œ [ì¸ì§€], [ì •ì„œ], [ë¬¼ë¦¬ì ì§€ì›] í‚¤ì›Œë“œê°€ ë¬¸ì¥ ì•ì— ì…ë ¥ë˜ì–´ ìˆë‹¤ë©´, ì„œë¹„ìŠ¤ëŠ” êµ¬ë¶„ëœ ìƒíƒœì…ë‹ˆë‹¤."
            "ê·¸ëŸ¬ë¯€ë¡œ ë” ì„œë¹„ìŠ¤ë¥¼ êµ¬ë¶„í•˜ì§€ ì•Šê³ , ì´ë¯¸ ì‘ë‹µí–ˆë˜ ë‚´ìš©ì˜ ëŒ€í™”ë¥¼ ì´ì–´ë‚˜ê°€ì•¼ í•©ë‹ˆë‹¤."
            "\n\n"
            "ì…ë ¥: {stt_text}\n\n"
            "ê²°ê³¼: "
        )
        self.chain = self.prompt | self.llm | self.output_parser

    def __call__(self, text: str) -> str:
        if not text:
            return ""
        return self.chain.invoke({"stt_text": text})


class SentenceCorrector:
    """
    """
    def __init__(self, model_name="gpt-4o-mini-2024-07-18"):
        """
        """
        # LLM í˜¸ì¶œ timeout ì„¤ì • (10ì´ˆ) - ROS2 ì„œë¹„ìŠ¤ ì‘ë‹µ ì§€ì—° ë°©ì§€
        self.llm = ChatOpenAI(
            model=model_name,
            timeout=10.0,  # 10ì´ˆ timeout
            max_retries=1
        )
        self.output_parser = StrOutputParser()
        self.prompt = PromptTemplate.from_template(
            "ë‹¤ìŒ ë¬¸ì¥ì€ ìŒì„± ì¸ì‹(STT)ì„ í†µí•´ ìë™ìœ¼ë¡œ ìƒì„±ëœ í…ìŠ¤íŠ¸ì…ë‹ˆë‹¤. "
            "ë„ì–´ì“°ê¸°, ë¬¸ë²• ì˜¤ë¥˜, ë¶€ìì—°ìŠ¤ëŸ¬ìš´ í‘œí˜„ì´ ìˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. "
            "ë¬¸ë§¥ì— ë§ê²Œ ìì—°ìŠ¤ëŸ½ê³  ì™„ì „í•œ ë¬¸ì¥ìœ¼ë¡œ ìˆ˜ì •í•´ì£¼ì„¸ìš”. "
            #"ë¬¸ì¥ìœ¼ë¡œ ìˆ˜ì • í›„, ìˆ˜ì •ëœ ë¬¸ì¥ì„ ì˜ì–´ë¡œ ë²ˆì—­í•´ì£¼ì„¸ìš”."
            "\n\n"
            "ì…ë ¥: {stt_text}\n\n"
            "ìˆ˜ì •ëœ ë¬¸ì¥:"
        )
        self.chain = self.prompt | self.llm | self.output_parser
        print("LangChain êµì •ê¸° ì´ˆê¸°í™” ì™„ë£Œ.")

    def correct(self, text: str) -> str:
        """
        text is corrected by LLM
        """
        if not text:
            return ""
        return self.chain.invoke({"stt_text": text})


<<<<<<< HEAD
#ì¶”ê°€
from langchain_openai import OpenAIEmbeddings

def get_llm(model_name: str = "gpt-4o-mini-2024-07-18", temperature: float = 0.4):
    """ê¸°ë³¸ LLM ê°ì²´ ë°˜í™˜ (LifeAssistMemoryì—ì„œ ì‚¬ìš©)"""
    return ChatOpenAI(
        model=model_name, 
        temperature=temperature,
        request_timeout=10,  # 10ì´ˆ íƒ€ì„ì•„ì›ƒ ì„¤ì • (ì„±ëŠ¥ í–¥ìƒ)
        max_retries=1  # 1ë²ˆë§Œ ì¬ì‹œë„ (ë¹ ë¥¸ fallback)
    )

def get_embedding():
    """ë²¡í„°ìŠ¤í† ì–´ìš© ì„ë² ë”© í•¨ìˆ˜"""
    return OpenAIEmbeddings()
#
=======
# =============================
# ğŸ”§ Compatibility Utilities
# =============================

from langchain_openai import ChatOpenAI, OpenAIEmbeddings

def get_llm(model_name: str = "gpt-4o-mini-2024-07-18"):
    """ê¸°ì¡´ ì½”ë“œ í˜¸í™˜ìš©: LangChain LLM ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
    return ChatOpenAI(
        model=model_name, 
        temperature=0.4,
        timeout=10.0,  # 10ì´ˆ timeout
        max_retries=1
    )

def get_embedding(model_name: str = "text-embedding-3-small"):
    """ê¸°ì¡´ ì½”ë“œ í˜¸í™˜ìš©: OpenAI Embedding ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
    return OpenAIEmbeddings(model=model_name)
>>>>>>> 9f3045d (2025-11-06 ìˆ˜ì • ì‚¬í•­ ë°˜ì˜)

import os
import pandas as pd
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Any, Optional
from collections import defaultdict
import logging
import threading
import time

logger = logging.getLogger(__name__)

import json
import re


def _normalize_schedule_title(title: str) -> str:
    """ì œëª© ë¹„êµìš©: strip í›„ ì—°ì† ê³µë°±ì„ í•˜ë‚˜ë¡œ. 'ë¯¸ìš©ì‹¤  ì˜ˆì•½' == 'ë¯¸ìš©ì‹¤ ì˜ˆì•½' """
    if not title or not isinstance(title, str):
        return ""
    return re.sub(r"\s+", " ", title.strip())


def _normalize_date_for_compare(date_val: str) -> str:
    """ë‚ ì§œ ë¹„êµìš©: '2026-02-26 00:00:00' ë˜ëŠ” '2026-02-26' -> '2026-02-26' """
    if not date_val:
        return ""
    s = str(date_val).strip()
    if len(s) >= 10 and s[4] == "-" and s[7] == "-":
        return s[:10]
    return s


class ScheduleConflictException(Exception):
    """ì¼ì • ì¶©ëŒ ì˜ˆì™¸: ë‚ ì§œ+ì‹œê°„ì´ ê°™ì€ë° ì œëª©ì´ ë‹¤ë¥¸ ì¼ì •ì´ ìˆì„ ë•Œ ë°œìƒ"""
    def __init__(self, existing_schedule: Dict[str, Any], new_schedule: Dict[str, Any], user_name: str):
        self.existing_schedule = existing_schedule
        self.new_schedule = new_schedule
        self.user_name = user_name
        super().__init__(f"ì¼ì • ì¶©ëŒ: {existing_schedule.get('ì œëª©', '')} vs {new_schedule.get('ì œëª©', '')}")

SHEET_SCHEMAS = {
    "ë¬¼ê±´ìœ„ì¹˜": ["ë‚ ì§œ", "ë¬¼ê±´ì´ë¦„", "ì¥ì†Œ", "ì„¸ë¶€ìœ„ì¹˜", "ì¶œì²˜", "ì—”í‹°í‹°íƒ€ì…"],
    "ë³µì•½ì •ë³´": ["ë‚ ì§œ", "ì•½ì´ë¦„", "ìš©ëŸ‰", "ë‹¨ìœ„", "ì‹œê°„", "ë³µìš©ë°©ë²•", "ë³µìš©ê¸°ê°„", "ì—”í‹°í‹°íƒ€ì…"],
    "ì¼ì •": ["ë‚ ì§œ", "ì œëª©", "ì‹œê°„", "ì¥ì†Œ", "ì •ë³´", "ì—”í‹°í‹°íƒ€ì…"],
    "ê°€ì¡±ê´€ê³„": ["ë‚ ì§œ", "ê´€ê³„", "ì´ë¦„", "ì •ë³´", "ì—”í‹°í‹°íƒ€ì…"],
    "ê°ì •ê¸°ë¡": ["ë‚ ì§œ", "ê°ì •", "ì •ë³´", "ì—”í‹°í‹°íƒ€ì…"],
    "ìŒì‹ê¸°ë¡": ["ë‚ ì§œ", "ë¼ë‹ˆ", "ì‹œê°„", "ë©”ë‰´", "ì—”í‹°í‹°íƒ€ì…"],
    "ì‚¬ìš©ìì •ë³´KV": ["ë‚ ì§œ", "í‚¤", "ê°’", "ì¶œì²˜", "í™•ì‹ ë„", "ì—”í‹°í‹°íƒ€ì…"],
    "ëŒ€í™”ê¸°ë¡": ["ë‚ ì§œ", "ì‹œê°„", "ëŒ€í™”ìš”ì•½"],
}

def _get_package_dir() -> Path:
    """user_excel_manager.pyì™€ ê°™ì€ íŒ¨í‚¤ì§€ ë””ë ‰í„°ë¦¬ (user_information ìƒìœ„)"""
    current_file = Path(__file__).resolve()
    return current_file.parent


def _get_user_information_dir() -> Path:
    """
    ë¶ˆëŸ¬ì˜¤ê¸°Â·ì €ì¥ ëª¨ë‘ ì†ŒìŠ¤ íŠ¸ë¦¬ì˜ user_information í•œ ê²½ë¡œë¡œ í†µì¼.
    - install ì‹¤í–‰ ì‹œ: .../src/life_assist_dm/life_assist_dm/user_information
    - ì†ŒìŠ¤ ì‹¤í–‰ ì‹œ: .../life_assist_dm/life_assist_dm/user_information
    """
    current_file = Path(__file__).resolve()
    parts = current_file.parts

    # install ê³µê°„ì—ì„œ ì‹¤í–‰ ì¤‘ì´ë©´ workspace/src/.../user_information ë¡œ ë§ì¶¤
    if "install" in parts:
        idx = parts.index("install")
        workspace = Path(*parts[:idx])
        source_user_info = workspace / "src" / "life_assist_dm" / "life_assist_dm" / "user_information"
        return source_user_info.resolve()

    # ì†ŒìŠ¤ì—ì„œ ì‹¤í–‰ ì‹œ: ì´ íŒŒì¼ ì˜†ì˜ user_information
    return (_get_package_dir() / "user_information").resolve()


class UserExcelManager:
    """ì—‘ì…€ ë¡œë“œ/ì €ì¥ì€ ë°˜ë“œì‹œ user_information ë””ë ‰í„°ë¦¬ í•œ ê³³ì—ì„œë§Œ ìˆ˜í–‰."""

    def __init__(self, base_dir: str = None):
        if base_dir is not None and str(base_dir).strip():
            self.base_dir = Path(os.path.expanduser(base_dir)).resolve()
        else:
            self.base_dir = _get_user_information_dir()
        self.base_dir.mkdir(parents=True, exist_ok=True)
        logger.info(f"ì—‘ì…€ ê²½ë¡œ [user_information]: {self.base_dir}")

        self._buffered_changes = defaultdict(list)

        self._flush_lock = threading.Lock()
        self._pending_flush = {}
        self._flush_delay = 1.0
    
    def _get_sheet_name(self, entity_type: str) -> str:
        mapping = {
            "ë¬¼ê±´": "ë¬¼ê±´ìœ„ì¹˜",
            "user.ë¬¼ê±´": "ë¬¼ê±´ìœ„ì¹˜",
            "ì•½": "ë³µì•½ì •ë³´",
            "user.ì•½": "ë³µì•½ì •ë³´",
            "ì¼ì •": "ì¼ì •",
            "user.ì¼ì •": "ì¼ì •",
            "ì‹ì‚¬": "ìŒì‹ê¸°ë¡",
            "user.ì‹ì‚¬": "ìŒì‹ê¸°ë¡",
            "ìŒì‹": "ìŒì‹ê¸°ë¡",
            "user.ìŒì‹": "ìŒì‹ê¸°ë¡",
            "ì •ì„œ": "ê°ì •ê¸°ë¡",
            "ê°ì •": "ê°ì •ê¸°ë¡",
            "user.ê±´ê°•ìƒíƒœ": "ê°ì •ê¸°ë¡",
            "ê°€ì¡±": "ê°€ì¡±ê´€ê³„",
            "user.ê°€ì¡±": "ê°€ì¡±ê´€ê³„",
            "ì‚¬ìš©ì": "ì‚¬ìš©ìì •ë³´KV",
            "user.ì‚¬ìš©ì": "ì‚¬ìš©ìì •ë³´KV",
            "ì·¨í–¥": "ì‚¬ìš©ìì •ë³´KV",
            "ì„ í˜¸": "ì‚¬ìš©ìì •ë³´KV",
            "ê¸°ë…ì¼": "ì‚¬ìš©ìì •ë³´KV",
            "ì·¨ë¯¸": "ì‚¬ìš©ìì •ë³´KV",
        }

        sheet_name = mapping.get(entity_type, "ì‚¬ìš©ìì •ë³´KV")
        if entity_type not in mapping:
            logger.info(f"[INFO] '{entity_type}' ì—”í‹°í‹° íƒ€ì…ì´ ë§¤í•‘ë˜ì§€ ì•Šì•„ ì‚¬ìš©ìì •ë³´KVë¡œ ì €ì¥")
        return sheet_name
        
    def get_user_excel_path(self, user_name: str) -> Path:

        file_name = f"{user_name}.xlsx"
        return self.base_dir / file_name
    
    def load_user_excel(self, user_name: str) -> Optional[pd.ExcelFile]:
        excel_path = self.get_user_excel_path(user_name)
        if not excel_path.exists():
            return None
        try:
            return pd.ExcelFile(excel_path)
        except Exception as e:
            logger.error(f"ì—‘ì…€ íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return None
    
    def load_sheet_data(self, user_name: str, sheet_name: str) -> pd.DataFrame:
        excel_file = self.load_user_excel(user_name)
        if excel_file is None:
            return pd.DataFrame()
        try:
            if sheet_name in excel_file.sheet_names:
                return pd.read_excel(excel_file, sheet_name=sheet_name)
            else:
                return pd.DataFrame()
        except Exception as e:
            logger.error(f"ì‹œíŠ¸ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨ ({sheet_name}): {e}")
            return pd.DataFrame()
    
    def safe_load_sheet(self, user_name: str, sheet_name: str) -> pd.DataFrame:
        try:
            df = self.load_sheet_data(user_name, sheet_name)
            schema = SHEET_SCHEMAS.get(sheet_name, [])
            if df is None or df.empty:
                return pd.DataFrame(columns=schema)

            for col in schema:
                if col not in df.columns:
                    df[col] = ""

            return df[schema]
        except Exception as e:
            logger.error(f"[ERROR] safe_load_sheet ì‹¤íŒ¨: {e}")
            schema = SHEET_SCHEMAS.get(sheet_name, [])
            return pd.DataFrame(columns=schema)
    
    def save_data_to_sheet(self, user_name: str, sheet_name: str, data: List[Dict[str, Any]], 
                           append: bool = True):
        excel_path = self.get_user_excel_path(user_name)
        
        def _cleanup_lockfile(path: Path):
            try:
                lock_path = Path(str(path) + ".lock")
                if lock_path.exists():
                    lock_path.unlink(missing_ok=True)
                    logger.debug(f"[LOCK CLEANUP] Lock íŒŒì¼ ì œê±°ë¨: {lock_path}")
            except Exception as e:
                logger.warning(f"[LOCK CLEANUP ì‹¤íŒ¨] {e}")
        
        existing_data = []
        if excel_path.exists() and append:
            try:
                df_existing = self.load_sheet_data(user_name, sheet_name)
                existing_data = df_existing.to_dict('records')
            except Exception as e:
                logger.warning(f"ê¸°ì¡´ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
        
        if append:
            existing_data.extend(data)
        else:
            existing_data = data
        
        df = pd.DataFrame(existing_data)

        schema = SHEET_SCHEMAS.get(sheet_name, [])
        if schema:

            for col in schema:
                if col not in df.columns:
                    df[col] = ""

            df = df[schema]
        
        try:
            mode = 'a' if excel_path.exists() else 'w'
            with pd.ExcelWriter(
                excel_path, engine='openpyxl', mode=mode, if_sheet_exists='replace'
            ) as writer:
                df.to_excel(writer, sheet_name=sheet_name, index=False)

            _cleanup_lockfile(excel_path)
        except TypeError:

            if excel_path.exists():
                excel_file = self.load_user_excel(user_name)
                if excel_file is None:
                    with pd.ExcelWriter(excel_path, engine='openpyxl', mode='w') as writer:
                        df.to_excel(writer, sheet_name=sheet_name, index=False)
                else:
                    excel_data = {}
                    for sheet in excel_file.sheet_names:
                        if sheet == sheet_name:
                            excel_data[sheet] = df
                        else:
                            excel_data[sheet] = pd.read_excel(excel_file, sheet_name=sheet)
                    if sheet_name not in excel_data:
                        excel_data[sheet_name] = df
                    with pd.ExcelWriter(excel_path, engine='openpyxl', mode='w') as writer:
                        for sheet_name_key, df_data in excel_data.items():
                            df_data.to_excel(writer, sheet_name=sheet_name_key, index=False)
                    _cleanup_lockfile(excel_path)
            else:
                with pd.ExcelWriter(excel_path, engine='openpyxl', mode='w') as writer:
                    df.to_excel(writer, sheet_name=sheet_name, index=False)
                _cleanup_lockfile(excel_path)
    
    def _convert_duration_to_date_range(self, duration_str: str) -> str:
        if not duration_str:
            return ""
        
        import re
        from datetime import datetime, timedelta
        
        if "~" in duration_str or "-" in duration_str:

            if re.match(r"\d{4}-\d{2}-\d{2}", duration_str.split("~")[0].strip()):
                return duration_str
        
        duration_match = re.search(r"(\d+)\s*(ì¼|ì£¼|ê°œì›”|ë…„)", duration_str)
        if not duration_match:

            return duration_str
        
        days_to_add = 0
        number = int(duration_match.group(1))
        unit = duration_match.group(2)
        
        if unit == "ì¼":
            days_to_add = number
        elif unit == "ì£¼":
            days_to_add = number * 7
        elif unit == "ê°œì›”":
            days_to_add = number * 30
        elif unit == "ë…„":
            days_to_add = number * 365
        
        start_date = datetime.now()
        end_date = start_date + timedelta(days=days_to_add - 1)
        
        start_str = start_date.strftime("%Y-%m-%d %H:%M:%S")
        end_str = end_date.strftime("%Y-%m-%d %H:%M:%S")
        return f"{start_str}~{end_str}"
    
    def _normalize_entity(self, entity_type: str, data: dict) -> dict:
        norm = {}
        try:
            if entity_type in ["ë¬¼ê±´", "user.ë¬¼ê±´"]:

                # ì›ë³¸ ì´ë¦„
                raw_name = data.get("ë¬¼ê±´ì´ë¦„") or data.get("ì´ë¦„", "")
                if isinstance(raw_name, str):
                    name = raw_name.strip()
                    # LLM í¬ë§·ì—ì„œ ì˜¨ "ì¹˜ì•½, location: null" ê°™ì€ ë…¸ì´ì¦ˆ ì œê±°
                    # - ", location: null" ì „ì²´ ì œê±°
                    # - "location: ..." íŒ¨í„´ì´ ì„ì—¬ ìˆìœ¼ë©´ ê·¸ ì•ê¹Œì§€ë¥¼ ë¬¼ê±´ ì´ë¦„ìœ¼ë¡œ ì‚¬ìš©
                    lower = name.lower()
                    if "location:" in lower:
                        # ì‰¼í‘œ ê¸°ì¤€ ì•ë¶€ë¶„ë§Œ ì‚¬ìš© (ì˜ˆ: "ì¹˜ì•½, location: null" â†’ "ì¹˜ì•½")
                        name = name.split("location:")[0].rstrip(", ").strip()
                    # í˜¹ì‹œ ë‚¨ì•„ ìˆì„ ìˆ˜ ìˆëŠ” "null" í† í° ì œê±°
                    if name.lower().endswith("null"):
                        name = name[:-4].rstrip(", ").strip()
                    norm["ë¬¼ê±´ì´ë¦„"] = name
                else:
                    norm["ë¬¼ê±´ì´ë¦„"] = str(raw_name) if raw_name is not None else ""

                norm["ì¥ì†Œ"] = str(data.get("ì¥ì†Œ", "")).strip()
                norm["ì„¸ë¶€ìœ„ì¹˜"] = str(data.get("ì„¸ë¶€ìœ„ì¹˜", "")).strip()
                
                # ì„¸ë¶€ìœ„ì¹˜ì—ì„œ ì¡°ì‚¬ ì œê±° (ì˜ˆ: "ìœ„ì—" â†’ "ìœ„", "ì•ì—ì„œ" â†’ "ì•")
                if norm["ì„¸ë¶€ìœ„ì¹˜"]:
                    import re
                    # ëì— ì˜¤ëŠ” ì¡°ì‚¬ ì œê±°: ì—, ì—ì„œ, ë¡œ, ìœ¼ë¡œ, ì˜, ì™€, ê³¼ ë“±
                    norm["ì„¸ë¶€ìœ„ì¹˜"] = re.sub(r'(ì—|ì—ì„œ|ë¡œ|ìœ¼ë¡œ|ì˜|ì™€|ê³¼|ê¹Œì§€|ë¶€í„°|ë§Œ|ë„|ì¡°ì°¨|ë§ˆì €|ë¶€í„°|ê¹Œì§€)$', '', norm["ì„¸ë¶€ìœ„ì¹˜"]).strip()

                if not norm["ì¥ì†Œ"] and not norm["ì„¸ë¶€ìœ„ì¹˜"]:
                    location = str(data.get("ìœ„ì¹˜", "")).strip()
                    if location:

                        import re

                        if "ë‚´ ë°©" in location or "ë‚´ë°©" in location:

                            if location.startswith("ë‚´ ë°©") or location.startswith("ë‚´ë°©"):

                                remaining = location.replace("ë‚´ ë°©", "").replace("ë‚´ë°©", "").strip()
                                norm["ì¥ì†Œ"] = "ë‚´ ë°©" if "ë‚´ ë°©" in location else "ë‚´ë°©"
                                norm["ì„¸ë¶€ìœ„ì¹˜"] = remaining
                            else:

                                norm["ì¥ì†Œ"] = "ë‚´ ë°©" if "ë‚´ ë°©" in location else "ë‚´ë°©"
                                norm["ì„¸ë¶€ìœ„ì¹˜"] = location.replace("ë‚´ ë°©", "").replace("ë‚´ë°©", "").strip()
                        else:

                            room_keywords = ["ì•ˆë°©", "ë‹¤ìš©ë„ì‹¤", "í™”ì¥ì‹¤", "ì£¼ë°©", "ê±°ì‹¤", "ì¹¨ì‹¤", "í˜„ê´€", "ë² ë€ë‹¤", "ë°©"]
                            room_keywords_sorted = sorted(room_keywords, key=len, reverse=True)
                            for room in room_keywords_sorted:
                                if room in location:
                                    norm["ì¥ì†Œ"] = room
                                    norm["ì„¸ë¶€ìœ„ì¹˜"] = location.replace(room, "").strip()
                                    break
                        if not norm["ì¥ì†Œ"]:

                            norm["ì„¸ë¶€ìœ„ì¹˜"] = location
                        
                        # location íŒŒì‹± í›„ì—ë„ ì„¸ë¶€ìœ„ì¹˜ì—ì„œ ì¡°ì‚¬ ì œê±°
                        if norm["ì„¸ë¶€ìœ„ì¹˜"]:
                            import re
                            norm["ì„¸ë¶€ìœ„ì¹˜"] = re.sub(r'(ì—|ì—ì„œ|ë¡œ|ìœ¼ë¡œ|ì˜|ì™€|ê³¼|ê¹Œì§€|ë¶€í„°|ë§Œ|ë„|ì¡°ì°¨|ë§ˆì €|ë¶€í„°|ê¹Œì§€)$', '', norm["ì„¸ë¶€ìœ„ì¹˜"]).strip()

                norm["ì¶œì²˜"] = data.get("ì¶œì²˜") or data.get("ì¶”ì¶œë°©ë²•", "ì‚¬ìš©ì ë°œí™”")

            elif entity_type in ["ì•½", "user.ì•½"]:

                norm["ì•½ì´ë¦„"] = data.get("ì•½ì´ë¦„") or data.get("ì•½ëª…") or data.get("ì´ë¦„", "")

                dose = str(data.get("ìš©ëŸ‰", "")).strip()
                unit = str(data.get("ë‹¨ìœ„", "")).strip()
                norm["ìš©ëŸ‰"] = dose if dose else ""
                norm["ë‹¨ìœ„"] = unit if unit else ""

                norm["ì‹œê°„"] = data.get("ì‹œê°„ëŒ€") or data.get("ì‹œê°„") or data.get("ë³µìš©ì‹œê°„", "")

                norm["ë³µìš©ë°©ë²•"] = data.get("ë³µìš©ë°©ë²•") or data.get("ë©”ëª¨") or ""

                ë³µìš©ê¸°ê°„_ì›ë³¸ = data.get("ë³µìš©ê¸°ê°„") or ""
                if ë³µìš©ê¸°ê°„_ì›ë³¸:

                    norm["ë³µìš©ê¸°ê°„"] = self._convert_duration_to_date_range(ë³µìš©ê¸°ê°„_ì›ë³¸)
                else:
                    norm["ë³µìš©ê¸°ê°„"] = ""
            elif entity_type == "ì¼ì •":
                norm["ì œëª©"] = data.get("ì œëª©", "")

                date_value = data.get("ë‚ ì§œ", "")
                if date_value:
                    try:
                        from life_assist_dm.support_chains import _normalize_date_to_iso
                        date_str = str(date_value).strip()
                        if date_str and date_str.lower() not in ("nan", "none", ""):
                            # ì¼ì • ì‹œíŠ¸ëŠ” ë‚ ì§œ ì»¬ëŸ¼ì„ ë‚ ì§œ(YYYY-MM-DD)ë§Œ ìœ ì§€
                            norm["ë‚ ì§œ"] = _normalize_date_to_iso(date_str)
                        else:
                            norm["ë‚ ì§œ"] = ""
                    except Exception as e:
                        logger.warning(f"ì¼ì • ë‚ ì§œ ì •ê·œí™” ì‹¤íŒ¨: {e}, ì›ë³¸ ê°’ ì‚¬ìš©: {date_value}")
                        norm["ë‚ ì§œ"] = str(date_value) if date_value else ""
                else:
                    norm["ë‚ ì§œ"] = ""
                norm["ì‹œê°„"] = data.get("ì‹œê°„", "")
                norm["ì¥ì†Œ"] = data.get("ì¥ì†Œ", "")
                norm["ì •ë³´"] = data.get("ì •ë³´", "")
            elif entity_type in ["ì‹ì‚¬", "ìŒì‹"]:
                norm["ë¼ë‹ˆ"] = data.get("ë¼ë‹ˆ", "")
                norm["ì‹œê°„"] = data.get("ì‹œê°„", "") or data.get("ì‹œê°„ëŒ€", "")
                if isinstance(data.get("ë©”ë‰´"), list):
                    norm["ë©”ë‰´"] = ", ".join(str(m) for m in data["ë©”ë‰´"])
                else:
                    norm["ë©”ë‰´"] = str(data.get("ë©”ë‰´", "")).strip()

                date_value = data.get("ë‚ ì§œ", "")
                if date_value:
                    try:
                        from life_assist_dm.support_chains import _normalize_date_to_iso
                        date_str = str(date_value).strip()
                        if date_str and date_str.lower() not in ("nan", "none", ""):
                            base = _normalize_date_to_iso(date_str)
                            norm["ë‚ ì§œ"] = f"{base} 00:00:00"
                        else:

                            norm["ë‚ ì§œ"] = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                    except Exception as e:
                        logger.warning(f"ë‚ ì§œ ì •ê·œí™” ì‹¤íŒ¨: {e}, ì›ë³¸ ê°’ ì‚¬ìš©: {date_value}")
                        norm["ë‚ ì§œ"] = str(date_value) if date_value else ""
                else:

                    norm["ë‚ ì§œ"] = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
            elif entity_type == "ì •ì„œ" or entity_type == "ê°ì •":
                norm["ê°ì •"] = data.get("ê°ì •") or data.get("ìƒíƒœ") or data.get("ì¦ìƒ", "")
                norm["ì •ë³´"] = data.get("ì •ë³´", "") or data.get("ì›ë¬¸", "")
            elif entity_type == "ê°€ì¡±":
                norm["ê´€ê³„"] = data.get("ê´€ê³„", "")
                norm["ì´ë¦„"] = data.get("ì´ë¦„", "")
                norm["ì •ë³´"] = data.get("ì •ë³´", "")
            elif entity_type in ["ì·¨í–¥", "ì„ í˜¸", "ê¸°ë…ì¼", "ì·¨ë¯¸"]:

                norm["ë‚´ìš©"] = data.get("ë‚´ìš©") or json.dumps(data, ensure_ascii=False)
                norm["ì •ë³´"] = data.get("ì •ë³´") or entity_type
            else:

                norm["ë‚´ìš©"] = json.dumps(data, ensure_ascii=False)
                norm["ì •ë³´"] = ""
        except Exception as e:
            logger.warning(f"ì—”í‹°í‹° ì •ê·œí™” ì¤‘ ì˜¤ë¥˜: {e}")
            norm["ë‚´ìš©"] = json.dumps(data, ensure_ascii=False)
            norm["ì •ë³´"] = ""
        return norm
    
    def save_entity_data(self, user_name: str, entity_type: str, data: Dict[str, Any]):

        if not user_name or not str(user_name).strip() or user_name == "ì‚¬ìš©ì":
            logger.warning(f"[WARN] ì˜ëª»ëœ ì‚¬ìš©ìëª…ìœ¼ë¡œ ì €ì¥ ì‹œë„: {user_name}")
            return
        if entity_type is None or (isinstance(entity_type, str) and entity_type.strip() in ("", "None")):
            logger.warning(f"[WARN] entity_typeì´ ë¹„ì–´ ìˆì–´ ì €ì¥ ìƒëµ: {entity_type!r}")
            return

        try:
            now = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
            sheet_name = self._get_sheet_name(entity_type)
            
            non_user_entity_types = ["ë¬¼ê±´", "ì•½", "ì¼ì •", "ì‹ì‚¬", "ìŒì‹", "ì •ì„œ", "ê°ì •", "ê°€ì¡±", "user.ë¬¼ê±´", "user.ì•½", "user.ì¼ì •", "user.ì‹ì‚¬", "user.ìŒì‹", "user.ê±´ê°•ìƒíƒœ", "user.ê°€ì¡±"]
            if entity_type not in non_user_entity_types:
                try:
                    from .dialog_manager.config.config_loader import get_excel_sheets
                    sheets = get_excel_sheets()
                    kv_sheet = sheets.get("user_info_kv", "ì‚¬ìš©ìì •ë³´KV")
                    
                    normalized_user = {}
                    import re
                    
                    if entity_type == "ì‚¬ìš©ì":

                        if "ë‚˜ì´" in data and data["ë‚˜ì´"]:
                            m = re.search(r"(\d+)", str(data["ë‚˜ì´"]))
                            if m:
                                normalized_user["ë‚˜ì´"] = f"{m.group(1)}ì‚´"

                        if "í•™êµ" in data and data["í•™êµ"]:
                            raw_school = str(data["í•™êµ"]).strip()
                            raw_school = re.sub(r"^(?:ë‚˜ëŠ”|ë‚œ|ì €ëŠ”)\s*", "", raw_school)
                            raw_school = re.sub(r"\s*(?:ì—\s*ë‹¤ë…€.*|ë‹¤ë…€.*)$", "", raw_school)
                            m = re.search(r"([ê°€-í£A-Za-z\s]+?(?:ì¤‘í•™êµ|ê³ ë“±í•™êµ|ëŒ€í•™êµ|ì´ˆë“±í•™êµ|í•™êµ))", raw_school)
                            if m:
                                normalized_user["í•™êµ"] = m.group(1).strip()

                        for k in ["ì´ë¦„", "ë³„ì¹­", "ì§ì—…", "ì·¨ë¯¸", "íšŒì‚¬", "ì¸í„´"]:
                            if k in data and data[k]:
                                normalized_user[k] = data[k]
                    elif entity_type in ["ì·¨í–¥", "ì„ í˜¸"]:

                        content = data.get("ë‚´ìš©", "") or data.get("ê°’", "") or json.dumps(data, ensure_ascii=False)
                        if content:
                            normalized_user["ì·¨í–¥"] = content
                    elif entity_type == "ê¸°ë…ì¼":

                        if "ì œëª©" in data and data["ì œëª©"]:
                            normalized_user["ê¸°ë…ì¼"] = f"{data.get('ì œëª©', '')} ({data.get('ë‚ ì§œ', '')})"
                        elif "ë‚ ì§œ" in data and data["ë‚ ì§œ"]:
                            normalized_user["ê¸°ë…ì¼"] = data.get("ë‚ ì§œ", "")
                    elif entity_type == "ì·¨ë¯¸":

                        hobby = data.get("ì´ë¦„", "") or data.get("ì·¨ë¯¸", "") or ""
                        if hobby:
                            normalized_user["ì·¨ë¯¸"] = hobby
                    else:

                        import json
                        entity_json = json.dumps(data, ensure_ascii=False)
                        normalized_user[entity_type] = entity_json
                    
                    kv_rows = []
                    if entity_type == "ì‚¬ìš©ì":
                        for k in ["ì´ë¦„", "ë³„ì¹­", "ë‚˜ì´", "í•™êµ", "ì§ì—…", "ì·¨ë¯¸", "íšŒì‚¬", "ì¸í„´"]:
                            if k in normalized_user and str(normalized_user[k]).strip() != "":
                                kv_rows.append({
                                    "ë‚ ì§œ": now,
                                    "í‚¤": k,
                                    "ê°’": normalized_user[k],
                                    "ì¶œì²˜": "ì‚¬ìš©ì ë°œí™”",
                                    "í™•ì‹ ë„": "",
                                    "ì—”í‹°í‹°íƒ€ì…": entity_type,
                                })
                    else:

                        for k, v in normalized_user.items():
                            if v and str(v).strip() != "":
                                kv_rows.append({
                                    "ë‚ ì§œ": now,
                                    "í‚¤": k,
                                    "ê°’": v,
                                    "ì¶œì²˜": "ì‚¬ìš©ì ë°œí™”",
                                    "í™•ì‹ ë„": "",
                                    "ì—”í‹°í‹°íƒ€ì…": entity_type,
                                })
                    if kv_rows:

                        self._buffered_changes[(user_name, kv_sheet)].extend(kv_rows)
                        logger.info(f"[BUFFER] {user_name}:{kv_sheet} ì—”í‹°í‹° ë²„í¼ë§ë¨ ({entity_type})")
                except Exception as e:

                    logger.error(f"[ERROR] ì‚¬ìš©ìì •ë³´KV ì €ì¥ ì‹¤íŒ¨: {e}")
                    try:
                        kv_sheet = "ì‚¬ìš©ìì •ë³´KV"
                        now_local = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                        kv_rows = []
                        for k in ["ì´ë¦„", "ë‚˜ì´", "í•™êµ", "ì§ì—…", "ì·¨ë¯¸", "íšŒì‚¬", "ì¸í„´"]:
                            if k in data and str(data[k]).strip() != "":
                                kv_rows.append({
                                    "ë‚ ì§œ": now_local,
                                    "í‚¤": k,
                                    "ê°’": data[k],
                                    "ì¶œì²˜": "ì‚¬ìš©ì ë°œí™”",
                                    "í™•ì‹ ë„": "",
                                    "ì—”í‹°í‹°íƒ€ì…": entity_type,
                                })
                        if kv_rows:
                            self._buffered_changes[(user_name, kv_sheet)].extend(kv_rows)
                            logger.info(f"[BUFFER] {user_name}:{kv_sheet} ì—”í‹°í‹° ë²„í¼ë§ë¨ ({entity_type})")
                    except Exception:
                        pass
                return
            
            normalized = self._normalize_entity(entity_type, data)

            date_value = normalized.get("ë‚ ì§œ", "")
            if sheet_name == "ì¼ì •":
                # ì¼ì • ì‹œíŠ¸: 'ë°œí™” ì‹œê°'ì´ ì•„ë‹Œ 'ì¼ì • ë‚ ì§œ'ë§Œ ì €ì¥
                # - ë°œí™”ì—ì„œ ë‚ ì§œê°€ ì¶”ì¶œëœ ê²½ìš°ì—ë§Œ YYYY-MM-DDë¡œ ì €ì¥
                # - ë‚ ì§œê°€ ì „í˜€ ì—†ìœ¼ë©´ ë¹ˆ ê°’ìœ¼ë¡œ ë‘ì–´, ì‚¬ìš©ìê°€ ë‚˜ì¤‘ì— ë³´ê±°ë‚˜ ìˆ˜ì •í•  ë•Œë„
                #   "ì–¸ì œ í•˜ëŠ” ì¼ì •ì¸ì§€ ëª¨ë¥´ëŠ” ì¼ì •"ìœ¼ë¡œ ëª…í™•íˆ ë³´ì´ê²Œ í•¨
                if date_value and str(date_value).strip().lower() not in ("nan", "none"):
                    dv = str(date_value).strip()
                    # í˜¹ì‹œ datetime í˜•íƒœê°€ ë“¤ì–´ì˜¤ë©´ ì•ì˜ ë‚ ì§œ ë¶€ë¶„ë§Œ ì‚¬ìš©
                    if len(dv) >= 10 and dv[4] == "-" and dv[7] == "-":
                        normalized["ë‚ ì§œ"] = dv[:10]
                    else:
                        normalized["ë‚ ì§œ"] = dv
                else:
                    normalized["ë‚ ì§œ"] = ""
            else:
                if not date_value or str(date_value).strip() == "" or str(date_value).lower() in ("nan", "none"):
                    # ê¸°ë³¸ì€ í•­ìƒ YYYY-MM-DD HH:MM:SS í¬ë§·
                    normalized["ë‚ ì§œ"] = now
                else:
                    dv = str(date_value).strip()
                    # YYYY-MM-DD í˜•íƒœë§Œ ë“¤ì–´ì˜¨ ê²½ìš°ì—ëŠ” ì‹œê°„ 00:00:00ì„ ë¶™ì—¬ì„œ í†µì¼
                    if len(dv) == 10 and dv[4] == "-" and dv[7] == "-":
                        normalized["ë‚ ì§œ"] = f"{dv} 00:00:00"
                    else:
                        normalized["ë‚ ì§œ"] = dv
            normalized["ì—”í‹°í‹°íƒ€ì…"] = entity_type
            
            schema = SHEET_SCHEMAS.get(sheet_name, SHEET_SCHEMAS["ì‚¬ìš©ìì •ë³´KV"])
            for col in schema:
                if col not in normalized:
                    normalized[col] = ""
            
            if entity_type in ["ë¬¼ê±´", "user.ë¬¼ê±´"]:
                logger.debug(f"[SAVE DEBUG] ë¬¼ê±´ ì €ì¥ - normalized: {normalized}")
                logger.debug(f"[SAVE DEBUG] ë¬¼ê±´ ì €ì¥ - schema: {schema}")
            
            record = {k: str(normalized[k]) if normalized[k] is not None else "" for k in schema}
            
            if entity_type in ["ë¬¼ê±´", "user.ë¬¼ê±´"]:
                logger.debug(f"[SAVE DEBUG] ë¬¼ê±´ ì €ì¥ - record: {record}")
            
            buffer_key = (user_name, sheet_name)
            
            # âœ… ì¼ì • ì¤‘ë³µ ì €ì¥ ë°©ì§€: ë‚ ì§œ + ì œëª© + ì‹œê°„ + ì¥ì†Œê°€ ëª¨ë‘ ê°™ìœ¼ë©´ ì¤‘ë³µ
            if entity_type in ["ì¼ì •", "user.ì¼ì •"]:
                # ë²„í¼ì—ì„œ ì¤‘ë³µ í™•ì¸
                existing_buffer = self._buffered_changes.get(buffer_key, [])
                duplicate_found = False
                
                record_date = record.get("ë‚ ì§œ", "").strip()
                record_title = record.get("ì œëª©", "").strip()
                record_time = record.get("ì‹œê°„", "").strip()
                record_location = record.get("ì¥ì†Œ", "").strip()
                record_date_norm = _normalize_date_for_compare(record_date)
                record_title_norm = _normalize_schedule_title(record_title)
                
                for i, existing_record in enumerate(existing_buffer):
                    existing_date = str(existing_record.get("ë‚ ì§œ", "")).strip()
                    existing_title = str(existing_record.get("ì œëª©", "")).strip()
                    existing_time = str(existing_record.get("ì‹œê°„", "")).strip()
                    existing_location = str(existing_record.get("ì¥ì†Œ", "")).strip()
                    existing_date_norm = _normalize_date_for_compare(existing_date)
                    existing_title_norm = _normalize_schedule_title(existing_title)
                    
                    # ë‚ ì§œ(ì¼ë§Œ), ì œëª©(ê³µë°± ì •ê·œí™”), ì‹œê°„, ì¥ì†Œê°€ ëª¨ë‘ ê°™ìœ¼ë©´ ì¤‘ë³µ
                    if (record_date_norm == existing_date_norm and 
                        record_title_norm == existing_title_norm and 
                        record_time == existing_time and 
                        record_location == existing_location):
                        # ê¸°ì¡´ ë ˆì½”ë“œë¥¼ ìƒˆ ê²ƒìœ¼ë¡œ êµì²´
                        existing_buffer[i] = record
                        duplicate_found = True
                        logger.info(f"[DUPLICATE] ì¼ì • ì¤‘ë³µ ë°œê²¬ - ê¸°ì¡´ ë ˆì½”ë“œ êµì²´: {record_title} ({record_date} {record_time})")
                        break
                
                if duplicate_found:
                    # ë²„í¼ ì—…ë°ì´íŠ¸ (ì´ë¯¸ êµì²´ë¨)
                    self._buffered_changes[buffer_key] = existing_buffer
                    logger.info(f"[BUFFER] {user_name}:{sheet_name} ì¼ì • ì¤‘ë³µ ë°©ì§€ - ê¸°ì¡´ ë ˆì½”ë“œ êµì²´ë¨ ({entity_type})")
                else:
                    # âœ… ë‚ ì§œ+ì‹œê°„ ì¶©ëŒ í™•ì¸ (ì œëª©ì´ ë‹¤ë¥¸ ê²½ìš°)
                    conflict_found = False
                    conflict_existing = None
                    
                    # ë²„í¼ì—ì„œ ë‚ ì§œ+ì‹œê°„ ì¶©ëŒ í™•ì¸ (ë‚ ì§œëŠ” ì¼ë§Œ, ì œëª©ì€ ê³µë°± ì •ê·œí™” í›„ ë¹„êµ)
                    for existing_record in existing_buffer:
                        existing_date = str(existing_record.get("ë‚ ì§œ", "")).strip()
                        existing_time = str(existing_record.get("ì‹œê°„", "")).strip()
                        existing_title = str(existing_record.get("ì œëª©", "")).strip()
                        existing_date_norm = _normalize_date_for_compare(existing_date)
                        existing_title_norm = _normalize_schedule_title(existing_title)
                        
                        # ë‚ ì§œ+ì‹œê°„ì´ ê°™ê³ , ì •ê·œí™”í•œ ì œëª©ì´ ë‹¤ë¥¼ ë•Œë§Œ ì¶©ëŒ
                        if (record_date_norm == existing_date_norm and 
                            record_time == existing_time and 
                            record_title_norm != existing_title_norm):
                            conflict_found = True
                            conflict_existing = existing_record
                            logger.info(f"[SCHEDULE CONFLICT] ë²„í¼ì—ì„œ ì¶©ëŒ ë°œê²¬: {existing_title} vs {record_title} ({record_date} {record_time})")
                            break
                    
                    # ì—‘ì…€ íŒŒì¼ì—ì„œë„ ë‚ ì§œ+ì‹œê°„ ì¶©ëŒ í™•ì¸
                    if not conflict_found:
                        try:
                            df_existing = self.load_sheet_data(user_name, sheet_name)
                            if df_existing is not None and not df_existing.empty:
                                for _, row in df_existing.iterrows():
                                    existing_date = str(row.get("ë‚ ì§œ", "")).strip()
                                    existing_title = str(row.get("ì œëª©", "")).strip()
                                    existing_time = str(row.get("ì‹œê°„", "")).strip()
                                    existing_location = str(row.get("ì¥ì†Œ", "")).strip()
                                    existing_date_norm = _normalize_date_for_compare(existing_date)
                                    existing_title_norm = _normalize_schedule_title(existing_title)
                                    
                                    # ë‚ ì§œ(ì¼ë§Œ)+ì œëª©(ì •ê·œí™”)+ì‹œê°„+ì¥ì†Œê°€ ëª¨ë‘ ê°™ìœ¼ë©´ ì™„ì „ ì¤‘ë³µ (ì €ì¥ ì•ˆ í•¨)
                                    if (record_date_norm == existing_date_norm and 
                                        record_title_norm == existing_title_norm and 
                                        record_time == existing_time and 
                                        record_location == existing_location):
                                        logger.info(f"[DUPLICATE] ì¼ì • ì¤‘ë³µ ë°œê²¬ (ì—‘ì…€) - ì €ì¥ ê±´ë„ˆëœ€: {record_title} ({record_date} {record_time})")
                                        return  # ì €ì¥í•˜ì§€ ì•Šê³  ì¢…ë£Œ
                                    
                                    # ë‚ ì§œ+ì‹œê°„ì´ ê°™ê³  ì •ê·œí™”í•œ ì œëª©ì´ ë‹¤ë¥¼ ë•Œë§Œ ì¶©ëŒ
                                    if (record_date_norm == existing_date_norm and 
                                        record_time == existing_time and 
                                        record_title_norm != existing_title_norm):
                                        conflict_found = True
                                        conflict_existing = {
                                            "ë‚ ì§œ": existing_date,
                                            "ì œëª©": existing_title,
                                            "ì‹œê°„": existing_time,
                                            "ì¥ì†Œ": existing_location,
                                            "ì •ë³´": str(row.get("ì •ë³´", "")).strip()
                                        }
                                        logger.info(f"[SCHEDULE CONFLICT] ì—‘ì…€ì—ì„œ ì¶©ëŒ ë°œê²¬: {existing_title} vs {record_title} ({record_date} {record_time})")
                                        break
                        except Exception as e:
                            logger.debug(f"[DUPLICATE CHECK] ì—‘ì…€ ì¤‘ë³µ í™•ì¸ ì‹¤íŒ¨ (ë¬´ì‹œ): {e}")
                    
                    # âœ… ì¶©ëŒì´ ìˆìœ¼ë©´ ì˜ˆì™¸ ë°œìƒ (í˜¸ì¶œí•˜ëŠ” ìª½ì—ì„œ ì²˜ë¦¬)
                    if conflict_found:
                        from life_assist_dm.user_excel_manager import ScheduleConflictException
                        raise ScheduleConflictException(
                            existing_schedule=conflict_existing,
                            new_schedule=record,
                            user_name=user_name
                        )
                    
                    # ì¤‘ë³µì´ ì—†ìœ¼ë©´ ë²„í¼ì— ì¶”ê°€
                    self._buffered_changes[buffer_key].append(record)
                    logger.info(f"[BUFFER] {user_name}:{sheet_name} ì—”í‹°í‹° ë²„í¼ë§ë¨ ({entity_type})")
            else:
                # ì¼ì •ì´ ì•„ë‹Œ ê²½ìš° ê¸°ì¡´ ë¡œì§ëŒ€ë¡œ ì¶”ê°€
                self._buffered_changes[buffer_key].append(record)
                logger.info(f"[BUFFER] {user_name}:{sheet_name} ì—”í‹°í‹° ë²„í¼ë§ë¨ ({entity_type})")
            
            logger.debug(f"[BUFFER DEBUG] ë²„í¼ë§ ì§í›„ - ë²„í¼ í‚¤: {buffer_key}, ë ˆì½”ë“œ ìˆ˜: {len(self._buffered_changes[buffer_key])}")
            
        except ScheduleConflictException:
            # ì¼ì • ì¶©ëŒì€ í˜¸ì¶œí•œ ìª½ì—ì„œ ì²˜ë¦¬í•´ì•¼ í•˜ë¯€ë¡œ ê·¸ëŒ€ë¡œ ì „ë‹¬
            raise
        except Exception as e:
            logger.error(f"[ERROR] save_entity_data ì‹¤íŒ¨: {e}")
            import traceback
            logger.error(traceback.format_exc())
    
    def save_conversation_summary(self, user_name: str, summary: str, 
                                 timestamp: Optional[str] = None):
        if timestamp is None:
            now = datetime.now()
            timestamp = now.strftime("%Y-%m-%d %H:%M:%S")
        
        record = {
            "ë‚ ì§œ": timestamp,
            "ì‹œê°„": timestamp.split()[1] if len(timestamp.split()) > 1 else "",
            "ëŒ€í™”ìš”ì•½": summary,
            "ì—”í‹°í‹°íƒ€ì…": "ëŒ€í™”ê¸°ë¡"
        }
        
        key = (user_name, "ëŒ€í™”ê¸°ë¡")
        self._buffered_changes[key].append(record)
        logger.info(f"[BUFFER] ëŒ€í™” ìš”ì•½ ë²„í¼ë§ë¨: {user_name}")

        try:
            if len(self._buffered_changes.get(key, [])) >= 3:
                self.request_flush(user_name)
                logger.info(f"[FLUSH] ëŒ€í™”ìš”ì•½ ëˆ„ì  3íšŒ â†’ Excel ë™ê¸°í™” ì˜ˆì•½ ({user_name})")
            else:
                logger.debug(f"[BUFFER] ëŒ€í™”ìš”ì•½ ëˆ„ì  {len(self._buffered_changes.get(key, []))}íšŒ (ë¯¸flush)")
        except Exception:
            pass
    
    def initialize_user_excel(self, user_name: str):
        excel_path = self.get_user_excel_path(user_name)
        
        with pd.ExcelWriter(excel_path, engine='openpyxl', mode='w') as writer:
            for sheet_name, columns in SHEET_SCHEMAS.items():
                df = pd.DataFrame(columns=columns)
                df.to_excel(writer, sheet_name=sheet_name, index=False)
        
        logger.info(f"ì‚¬ìš©ì ì—‘ì…€ íŒŒì¼ ì´ˆê¸°í™” ì™„ë£Œ: {user_name}")
    
    def user_exists(self, user_name: str) -> bool:
        return self.get_user_excel_path(user_name).exists()

    def cleanup_all_locks(self):
        try:
            for lockfile in self.base_dir.glob("*.xlsx.lock"):
                try:
                    lockfile.unlink(missing_ok=True)
                    logger.debug(f"[LOCK CLEANUP] ì„¸ì…˜ ì¢…ë£Œ ì „ ì œê±°ë¨: {lockfile}")
                except Exception as e:
                    logger.warning(f"[LOCK CLEANUP ì‹¤íŒ¨] {e}")
        except Exception as e:
            logger.warning(f"[LOCK CLEANUP ìŠ¤ìº” ì‹¤íŒ¨] {e}")
    
    def request_flush(self, user_name: str, delay: float = None):
        """
        flush_to_excel()ì„ ë°”ë¡œ ì‹¤í–‰í•˜ì§€ ì•Šê³ , ì•½ê°„ ì§€ì—°ì‹œì¼œ
        ë™ì‹œì— ì—¬ëŸ¬ ìš”ì²­ì´ ë“¤ì–´ì˜¬ ë•Œ í•œ ë²ˆë§Œ ì‹¤í–‰ë˜ê²Œ ë³‘í•©í•œë‹¤.
        
        Args:
            user_name: ì‚¬ìš©ì ì´ë¦„
            delay: ì§€ì—° ì‹œê°„ (ì´ˆ), Noneì´ë©´ ê¸°ë³¸ê°’(self._flush_delay) ì‚¬ìš©
        """
        if delay is None:
            delay = self._flush_delay
        
        if self._pending_flush.get(user_name, False):
            logger.debug(f"[FLUSH REQUEST] {user_name} - ì´ë¯¸ ì˜ˆì•½ëœ flush ìˆìŒ - ë³‘í•©ë¨")
            return
        
        self._pending_flush[user_name] = True
        logger.debug(f"[FLUSH REQUEST] {user_name} - flush ì˜ˆì•½ë¨ - {delay:.1f}ì´ˆ í›„ ì‹¤í–‰ ì˜ˆì •")
        
        def _delayed_flush():
            try:
                time.sleep(delay)
                with self._flush_lock:
                    logger.debug(f"[FLUSH THREAD] {user_name} - ì‹¤í–‰ ì‹œì‘")
                    self.flush_to_excel(user_name)
            except Exception as e:
                logger.error(f"[FLUSH THREAD ERROR] {user_name} - {e}")
                import traceback
                logger.error(traceback.format_exc())
            finally:
                self._pending_flush[user_name] = False
                logger.debug(f"[FLUSH THREAD] {user_name} - ì‹¤í–‰ ì™„ë£Œ")
        
        threading.Thread(target=_delayed_flush, daemon=True).start()
    
    def flush_to_excel(self, user_name: str):
        excel_path = self.get_user_excel_path(user_name)
        
        try:

            logger.info(f"[FLUSH DEBUG] flush ì‹œì‘ - ì „ì²´ ë²„í¼ í‚¤: {list(self._buffered_changes.keys())}")
            
            user_buffers = {k: v for k, v in self._buffered_changes.items() if k[0] == user_name}
            
            logger.info(f"[FLUSH DEBUG] {user_name} ë²„í¼ ìƒíƒœ: {[(k, len(v)) for k, v in user_buffers.items()]}")
            
            if not user_buffers:
                logger.debug(f"[FLUSH] {user_name} ë²„í¼ê°€ ë¹„ì–´ìˆìŒ")
                return
            
            excel_file = self.load_user_excel(user_name)
            excel_data = {}
            
            if excel_file:

                for sheet in excel_file.sheet_names:
                    excel_data[sheet] = self.safe_load_sheet(user_name, sheet)
            else:

                for sheet_name in SHEET_SCHEMAS.keys():
                    excel_data[sheet_name] = pd.DataFrame(columns=SHEET_SCHEMAS[sheet_name])
            
            for (uname, sheet_name), records in user_buffers.items():

                logger.debug(f"[FLUSH DEBUG] ì²˜ë¦¬ ì¤‘: ì‹œíŠ¸={sheet_name}, ë ˆì½”ë“œ ìˆ˜={len(records) if records else 0}")
                
                if not records:
                    logger.debug(f"[FLUSH DEBUG] ê±´ë„ˆëœ€: ì‹œíŠ¸={sheet_name} (ë ˆì½”ë“œ ì—†ìŒ)")
                    continue
                
                try:
                    schema = SHEET_SCHEMAS.get(sheet_name, SHEET_SCHEMAS["ì‚¬ìš©ìì •ë³´KV"])

                    ordered_records = []
                    for record in records:
                        ordered_record = {col: str(record.get(col, "")).strip() if record.get(col) is not None else "" for col in schema}
                        ordered_records.append(ordered_record)
                    
                    df_new = pd.DataFrame(ordered_records, columns=schema)
                    
                    if sheet_name == "ë¬¼ê±´ìœ„ì¹˜":
                        logger.debug(f"[FLUSH DEBUG] ë¬¼ê±´ìœ„ì¹˜ DataFrame:\n{df_new.head()}")
                        logger.debug(f"[FLUSH DEBUG] ë¬¼ê±´ìœ„ì¹˜ ì»¬ëŸ¼ ìˆœì„œ: {list(df_new.columns)}")
                except Exception as e:
                    logger.error(f"[FLUSH ERROR] DataFrame ìƒì„± ì‹¤íŒ¨: ì‹œíŠ¸={sheet_name}, ì˜¤ë¥˜={e}, ë ˆì½”ë“œ={records}")
                    import traceback
                    logger.error(traceback.format_exc())
                    continue
                
                if sheet_name in excel_data:
                    df_existing = excel_data[sheet_name]
                    df_all = pd.concat([df_existing, df_new], ignore_index=True)
                else:
                    df_all = df_new

                # âœ… ì¼ì • ì‹œíŠ¸ ì²˜ë¦¬:
                #   - ì¤‘ë³µ ì œê±°: ë‚ ì§œ(ì¼ë§Œ) + ì œëª©(ê³µë°± ì •ê·œí™”) + ì‹œê°„ + ì¥ì†Œê°€ ê°™ìœ¼ë©´ ì¤‘ë³µ
                #   - 'ë‚ ì§œ' ì»¬ëŸ¼ì€ ë°œí™” ì‹œê°„ì´ ì•„ë‹Œ 'ì¼ì • ë‚ ì§œ(YYYY-MM-DD)'ë§Œ ì €ì¥
                if sheet_name == "ì¼ì •" and not df_all.empty:
                    try:
                        if all(col in df_all.columns for col in ["ë‚ ì§œ", "ì œëª©", "ì‹œê°„", "ì¥ì†Œ"]):
                            for col in ["ë‚ ì§œ", "ì œëª©", "ì‹œê°„", "ì¥ì†Œ"]:
                                df_all[col] = df_all[col].fillna("").astype(str).str.strip()
                            # ì œëª© ê³µë°± ì •ê·œí™” / ë‚ ì§œ(ì¼ë§Œ) ë¹„êµìš© ì»¬ëŸ¼
                            df_all["_ì œëª©_norm"] = df_all["ì œëª©"].apply(lambda x: _normalize_schedule_title(str(x)))
                            df_all["_ë‚ ì§œ_norm"] = df_all["ë‚ ì§œ"].apply(lambda x: _normalize_date_for_compare(str(x)))

                            # ğŸ”¹ ê³¼ê±° ë²„ê·¸ë¡œ ì¸í•´ ê°™ì€ ì œëª©/ì‹œê°„/ì¥ì†Œì¸ë° ë‚ ì§œê°€ ë¹„ì–´ ìˆëŠ” í–‰ê³¼ ì±„ì›Œì§„ í–‰ì´ ê°™ì´ ìˆëŠ” ê²½ìš°
                            #    â†’ ë‚ ì§œê°€ ì±„ì›Œì§„ í–‰ë§Œ ë‚¨ê¸°ê³ , ë¹„ì–´ ìˆëŠ” í–‰ì€ ì •ë¦¬
                            try:
                                to_drop_idx = []
                                group_cols = ["_ì œëª©_norm", "ì‹œê°„", "ì¥ì†Œ"]
                                for _, group in df_all.groupby(group_cols, dropna=False):
                                    has_non_empty = (group["_ë‚ ì§œ_norm"] != "").any()
                                    has_empty = (group["_ë‚ ì§œ_norm"] == "").any()
                                    if has_non_empty and has_empty:
                                        empty_idx = group.index[group["_ë‚ ì§œ_norm"] == ""].tolist()
                                        to_drop_idx.extend(empty_idx)
                                if to_drop_idx:
                                    df_all = df_all.drop(index=to_drop_idx)
                            except Exception:
                                pass

                            # ì •ë ¬ì€ datetimeìœ¼ë¡œ í•˜ë˜, ìµœì¢… ì €ì¥ì€ '_ë‚ ì§œ_norm'(YYYY-MM-DD ë˜ëŠ” ë¹ˆ ë¬¸ìì—´)ë§Œ ìœ ì§€
                            if "_ë‚ ì§œ_norm" in df_all.columns:
                                _dt = pd.to_datetime(df_all["_ë‚ ì§œ_norm"], errors="coerce")
                                df_all = df_all.assign(_ë‚ ì§œ_dt=_dt)
                                df_all = df_all.sort_values("_ë‚ ì§œ_dt", na_position="last")

                            df_all = df_all.drop_duplicates(subset=["_ë‚ ì§œ_norm", "_ì œëª©_norm", "ì‹œê°„", "ì¥ì†Œ"], keep="last")

                            # ìµœì¢…ì ìœ¼ë¡œ 'ë‚ ì§œ'ëŠ” '_ë‚ ì§œ_norm'(YYYY-MM-DD ë˜ëŠ” ë¹ˆ ë¬¸ìì—´)ë¡œ ì €ì¥
                            if "ë‚ ì§œ" in df_all.columns and "_ë‚ ì§œ_norm" in df_all.columns:
                                df_all["ë‚ ì§œ"] = df_all["_ë‚ ì§œ_norm"]
                            df_all = df_all.drop(columns=["_ì œëª©_norm", "_ë‚ ì§œ_norm", "_ë‚ ì§œ_dt"], errors="ignore")

                            logger.debug(f"[DUPLICATE CHECK] ì¼ì • ì¤‘ë³µ ì œê±° ë° ë‚ ì§œ í˜•ì‹ ì •ë¦¬ ì™„ë£Œ: {len(df_all)}ê°œ ë ˆì½”ë“œ")
                    except Exception as e:
                        logger.warning(f"[DUPLICATE CHECK] ì¼ì • ì¤‘ë³µ ì œê±°/ë‚ ì§œ ì •ë¦¬ ì‹¤íŒ¨: {e}")
                
                if sheet_name == "ë¬¼ê±´ìœ„ì¹˜" and not df_all.empty:
                    try:
                        # ë™ì¼ ë¬¼ê±´ì— ëŒ€í•´ ê°€ì¥ ìµœì‹  ìœ„ì¹˜ë§Œ ìœ ì§€
                        # - ë™ì¼ ë¬¼ê±´ + ë‹¤ë¥¸ ìœ„ì¹˜: ê¸°ì¡´ í–‰ ì‚­ì œ, ìƒˆ í–‰ìœ¼ë¡œ ë®ì–´ì“°ê¸°
                        # - ë™ì¼ ë¬¼ê±´ + ë™ì¼ ìœ„ì¹˜: ì¤‘ë³µ ì œê±° (keep last)
                        if "ë¬¼ê±´ì´ë¦„" in df_all.columns:
                            for col in ["ë¬¼ê±´ì´ë¦„", "ì¥ì†Œ", "ì„¸ë¶€ìœ„ì¹˜"]:
                                if col in df_all.columns:
                                    df_all[col] = df_all[col].fillna("").astype(str).str.strip()

                            # ë‚ ì§œ ì»¬ëŸ¼ì´ ìˆìœ¼ë©´ datetimeìœ¼ë¡œ ë³€í™˜ í›„ ì •ë ¬
                            if "ë‚ ì§œ" in df_all.columns:
                                df_all["ë‚ ì§œ"] = pd.to_datetime(df_all["ë‚ ì§œ"], errors="coerce")
                                df_all = df_all.sort_values("ë‚ ì§œ", na_position="last")
                            else:
                                logger.debug("[DUPLICATE CHECK] 'ë‚ ì§œ' ì»¬ëŸ¼ ì—†ìŒ â†’ ì •ë ¬ ì—†ì´ ì¤‘ë³µ ì œê±° ìˆ˜í–‰")

                            # ë™ì¼ ë¬¼ê±´ì´ë¦„ ê¸°ì¤€ìœ¼ë¡œ ë§ˆì§€ë§‰(ìµœì‹ ) í–‰ë§Œ ë‚¨ê¹€
                            df_all = df_all.drop_duplicates(subset=["ë¬¼ê±´ì´ë¦„"], keep="last")

                            logger.debug(f"[DUPLICATE CHECK] ë¬¼ê±´ìœ„ì¹˜ ì¤‘ë³µ ì œê±° í›„: {len(df_all)}ê°œ ë ˆì½”ë“œ")
                    except Exception as e:
                        logger.warning(f"[FLUSH WARN] ë¬¼ê±´ìœ„ì¹˜ ì¤‘ë³µ ì œê±° ì‹¤íŒ¨: {e}")
                        import traceback
                        logger.debug(traceback.format_exc())

                if sheet_name == "ë³µì•½ì •ë³´" and not df_all.empty:
                    try:

                        if all(col in df_all.columns for col in ["ì•½ì´ë¦„", "ì‹œê°„", "ë³µìš©ë°©ë²•", "ë³µìš©ê¸°ê°„"]):

                            for col in ["ì•½ì´ë¦„", "ì‹œê°„", "ë³µìš©ë°©ë²•", "ë³µìš©ê¸°ê°„"]:
                                df_all[col] = df_all[col].fillna("").astype(str).str.strip()
                            
                            df_all = df_all.sort_values("ë‚ ì§œ", na_position='last')
                            
                            logger.debug(f"[DUPLICATE CHECK] ë³µì•½ì •ë³´ ì¤‘ë³µ ì œê±° ì „: {len(df_all)}ê°œ ë ˆì½”ë“œ")
                            logger.debug(f"[DUPLICATE CHECK] ìƒ˜í”Œ ë°ì´í„°:\n{df_all[['ì•½ì´ë¦„', 'ì‹œê°„', 'ë³µìš©ë°©ë²•', 'ë³µìš©ê¸°ê°„']].head()}")
                            
                            df_all = df_all.drop_duplicates(
                                subset=["ì•½ì´ë¦„", "ì‹œê°„", "ë³µìš©ë°©ë²•", "ë³µìš©ê¸°ê°„"],
                                keep="last"
                            )
                            
                            logger.debug(f"[DUPLICATE CHECK] ë³µì•½ì •ë³´ ì¤‘ë³µ ì œê±° í›„: {len(df_all)}ê°œ ë ˆì½”ë“œ")
                            logger.debug(f"[FLUSH] ë³µì•½ì •ë³´ ì¤‘ë³µ ì œê±° ì™„ë£Œ: {len(df_new)}ê°œ ì¶”ê°€ â†’ {len(df_all)}ê°œ ìµœì¢…")
                    except Exception as e:
                        logger.warning(f"[FLUSH WARN] ë³µì•½ì •ë³´ ì¤‘ë³µ ì œê±° ì‹¤íŒ¨: {e}")
                        import traceback
                        logger.debug(traceback.format_exc())

                if sheet_name == "ì‚¬ìš©ìì •ë³´KV" and not df_all.empty:
                    try:

                        if "ë‚ ì§œ" in df_all.columns and "í‚¤" in df_all.columns:
                            df_all = df_all.sort_values("ë‚ ì§œ").drop_duplicates(subset=["í‚¤"], keep="last")
                    except Exception as _:
                        pass
                
                excel_data[sheet_name] = df_all

                logger.info(f"[FLUSH] {user_name}:{sheet_name} â†’ {len(df_new)}ê°œ ë ˆì½”ë“œ ì €ì¥ ì™„ë£Œ")
            
            with pd.ExcelWriter(excel_path, engine='openpyxl', mode='w') as writer:
                for sheet_name, df_data in excel_data.items():
                    df_data.to_excel(writer, sheet_name=sheet_name, index=False)
            
            keys_to_remove = [k for k in self._buffered_changes.keys() if k[0] == user_name]
            for k in keys_to_remove:
                del self._buffered_changes[k]
            
            remaining_buffers = [k for k in self._buffered_changes.keys() if k[0] == user_name]
            logger.info(f"[FLUSH SUMMARY] {user_name} ë²„í¼ ìƒíƒœ: {remaining_buffers if remaining_buffers else 'ë¹„ì–´ìˆìŒ'}")
            logger.info(f"[FLUSH] {user_name} ë²„í¼ â†’ ì—‘ì…€ ë™ê¸°í™” ì™„ë£Œ")
                
        except Exception as e:
            logger.error(f"[ERROR] flush_to_excel ì‹¤íŒ¨: {e}")
            import traceback
            logger.error(traceback.format_exc())

    def save_entity(self, user_name: str, entity_type: str, entity_data: Dict[str, Any]):
        try:
            self.save_entity_data(user_name, entity_type, entity_data)
            logger.info(f"[BUFFER] {user_name}:{self._get_sheet_name(entity_type)} ì—”í‹°í‹° ë²„í¼ë§ë¨ ({entity_type})")
        except Exception as e:
            logger.error(f"[ERROR] save_entity ì‹¤íŒ¨: {e}")
